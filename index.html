<!DOCTYPE html>
<html >
<head>
  <!-- Site made with Mobirise Website Builder v4.8.5, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v4.8.5, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/logo4.png" type="image/x-icon">
  <meta name="description" content="">
  <title>Turing OC</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/socicon/css/styles.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">

  <link rel="stylesheet" type ="text/css" href="assets/style.css">
</head>

<body>
  <section class="cid-r6D4MGw6Co mbr-fullscreen mbr-parallax-background" id="header2-7">

    <div class="container align-center">
        <div class="row justify-content-md-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title mbr-bold pb-3 mbr-fonts-style display-1">
                    TURING OPTIMIZATION CLUB</h1>

                <p class="mbr-text pb-3 mbr-fonts-style display-5"> <b>Upcoming meeting </b>: <br> 12:30 - 2pm <br> Isaac Asimov and Mary Shelley rooms <br> The Alan Turing Institute, London </p>

            </div>
        </div>
    </div>
    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="timeline1 cid-r6DfrWv5Gj" id="timeline1-h">
  <div class="container">

    <h2 class="mbr-section-title pb-3 mbr-fonts-style display-2">
      <center>
        Past and Upcoming events
      </center>
    </h2>
   <div class="row">

      <div class="col-md-12 col-lg-12">
         <div id="tracking-pre"></div>
         <div id="tracking">
          <!--  <div class="text-center tracking-status-intransit">
               <p class="tracking-status text-tight">in transit</p>
            </div> -->

            <div class="tracking-list">

              <div class="tracking-item">
                  <div class="tracking-icon status-live">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Nov 11, 2019<span>12:30</span></div>
                  <div class="tracking-content">
                    <b> Shenglong Zhou </b>
                    <span> University of Southampton</span>
                    <b>Title</b>: Bilevel Optimisation with Applications into Hyper-parameter Tuning in Machine Learning
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_13" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_13" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                      </p>
                  </div>
              </div>
              </div>

              <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Mar 12, 2019<span>10:30</span></div>
                  <div class="tracking-content">
                    <b> Dominic Richards </b>
                    <span> University of Oxford</span>
                    <b>Title</b>: Optimal Statistical Rates for Non-parametric Decentralised Regression with Distributed Gradient Descent
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_12" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_12" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        Due to bandwidth limitations, privacy concerns or network instability, it is often required to fit statistical models on data sets stored across multiple computers without a central server to coordinate computation and disseminate information i.e. star topology. This has motivated the investigation of decentralised methods which solve the problem in a more robust manner by not relying on a single computer. In this talk we investigate the statistical performance of a simple synchronous decentralised iterative gradient descent method (Distributed Gradient Descent) in the homogeneous distributed non-parametric regression setting i.e. computers hold samples from the same distribution. By utilising the concentration of quantities held by individual computers, we show there are a number of settings where computers can save on computational and communication costs without any loss in statistical accuracy. Given computers hold sufficiently many samples with respect to the network topology, we show that Distributed Gradient Descent yields optimal statistical rates with the same numbers of iterations as Centralised algorithm. (Joint work with P. Rebeschini)
                      </p>
                  </div>
              </div>
              </div>

             <div class="tracking-item">
                 <div class="tracking-icon status-intransit">
                    <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                       <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                    </svg>
                    <!-- <i class="fas fa-circle"></i> -->
                 </div>
                 <div class="tracking-date">Feb 12, 2019<span>10:30</span></div>
                 <div class="tracking-content">
                   <b> Erlend S Riis </b>
                   <span> University of Cambridge</span>
                   <b>Title</b>: A geometric integration approach to nonsmooth, nonconvex optimisation
                 </div>
                 <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_11" aria-expanded="false" aria-controls="collapse1">
                     <b>
                         <font color="blue"> Abstract </font>
                     </b>
                 </a>
              <div id="collapse1_11" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                 <div class="panel-body p-4">
                     <p class="mbr-fonts-style panel-text display-7">
                       Discrete gradient methods are popular numerical methods from geometric integration for solving systems of ODEs. They are well-known for preserving structures of the continuous system such as energy dissipation/conservation. The preservation of dissipation makes discrete gradient methods interesting for optimisation problems. In this talk, we consider a derivative-free discrete gradient applied to dissipative ODEs such as gradient flow, thereby obtaining optimisation schemes that simultaneously are implementable in a black-box setting and retain favourable properties of gradient flow. We give a theoretical analysis of these schemes in the nonsmooth, nonconvex setting, and conclude with numerical results for the bilevel optimisation of regularisation parameters in image processing.
                     </p>
                 </div>
             </div>
             </div>


           <div class="tracking-item">
               <div class="tracking-icon status-intransit">
                  <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                     <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                  </svg>
                  <!-- <i class="fas fa-circle"></i> -->
               </div>
               <div class="tracking-date">Jan 29, 2019<span>10:30</span></div>
               <div class="tracking-content">
                 <b> Giuseppe Ughi</b>
                 <span> University of Oxford</span>
                 <b>Title</b>: Derivative Free Optimisation for Generating Adversarial Examples
               </div>
               <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_10" aria-expanded="false" aria-controls="collapse1">
                   <b>
                       <font color="blue"> Abstract </font>
                   </b>
               </a>
            <div id="collapse1_10" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
               <div class="panel-body p-4">
                   <p class="mbr-fonts-style panel-text display-7">
                     Neural Network algorithms have achieved unprecedented performance in image recognition over the past decade. However, their application to real world applications, such as self driving cars, raises the question of whether it is safe to rely on them.
                   </p>
                   <p class="mbr-fonts-style panel-text display-7">
                     We generally associate the robustness of these algorithms with how easy it is to generate an adversarial example: a tiny perturbation of an image which leads it to be misclassified by the Neural Net (which classifies the original image correctly). In this talk, we consider the generation of adversarial examples when the architecture of the target neural net is unknown. This leads us to consider the solution of the problem via derivative free optimisation methods. In this talk we will introduce the generation of the adversarial image through a model-based method and compare this to results achieved in the previous years.
                   </p>
               </div>
           </div>
           </div>
           </div>

            <div class="tracking-list">

             <div class="tracking-item">
                 <div class="tracking-icon status-intransit">
                    <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                       <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                    </svg>
                    <!-- <i class="fas fa-circle"></i> -->
                 </div>
                 <div class="tracking-date">Nov 21, 2018<span>10:30</span></div>
                 <div class="tracking-content">
                   <b> Amartya Sanyal </b>
                   <span> University of Oxford and The Alan Turing Institute</span>
                   <b>Title</b>: Intriguing Properties of Learned Representations
                 </div>
                 <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_9" aria-expanded="false" aria-controls="collapse1">
                     <b>
                         <font color="blue"> Abstract </font>
                     </b>
                 </a>
              <div id="collapse1_9" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                 <div class="panel-body p-4">
                     <p class="mbr-fonts-style panel-text display-7">
                         A key feature of neural networks, particularly deep convolutional neural networks, is their ability to learn useful representations from data. The very last layer of a neural network is then simply a linear model trained on these learned representations. Despite their numerous applications in other tasks such as classification, retrieval, clustering etc., a.k.a. transfer learning, not much work has been published that investigates the structure of these representations or indeed whether structure can be imposed on them during the training process.
                     </p>
                     <p class="mbr-fonts-style panel-text display-7">
                         In this paper, we study the effective dimensionality of the learned representations by models that have proved highly successful for image classification. We focus on ResNet-18, ResNet-50 and VGG-19 and observe that when trained on CIFAR10 or CIFAR100, the learned representations exhibit a fairly low rank structure. We propose a modification to the training procedure, which further encourages low rank structure on learned activations. Empirically, we show that this has implications for robustness to adversarial examples and compression.
                     </p>
                 </div>
             </div>
             </div>

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Nov 7, 2018<span>10:30</span></div>
                  <div class="tracking-content">
                    <b> Stéphane Chrétien </b>
                    <span> National Physical Laboratory</span>
                    <b>Title</b>: Who's afraid of the incremental gradient method?
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_8" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_8" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">

                      </p>
                  </div>
              </div>
              </div>

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Jun 25, 2018<span>14:00</span></div>
                  <div class="tracking-content">
                    <b> Stephen Wright</b>
                    <span>University of Wisconsin, Madison</span>
                    <b>Title</b>: Practical Nonconvex Optimization Algorithms With Complexity Guarantees.
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_7" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_7" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        Several widely used paradigms in data analysis can be formulated as
                        smooth nonconvex optimization problems with many variables. Classical
                        algorithms for this problem include L-BFGS, nonlinear conjugate gradient,
                        and Newton-conjugate gradient ("Newton-CG"). Alternative approaches
                        have been proposed more recently, motivated more by favorable worst-case
                        complexity guarantees than by practical performance. After surveying the
                        data science context, we present a variant of Newton-CG for which worst-case
                        complexity guarantees can be proved. We present computational results for a
                        variety of algorithms on several applications from data science, including robust
                        statistics and matrix and tensor completion. This talk is based in part on joint
                        research with Clement Royer and Michael O'Neill.
                      </p>
                  </div>
              </div>
              </div>

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Mar 21, 2018<span>11:00</span></div>
                  <div class="tracking-content">
                    <b> Jingwei Liang</b>
                    <span>University of Cambridge</span>
                    <b>Title</b>: Identifiability of Forward—Backward-type Splitting Methods.
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_6" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_6" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        Forward-Backward splitting method, a.k.a. proximal gradient descent is one of the most widely used algorithms in the literature. Over the decades, many variants of the method are proposed under different application scenarios, such that the accelerated FISTA scheme for image processing and the stochastic Forward—Backward for learning tasks. In this talk, I will talk about the local property of the class of Forward-Backward splitting methods and show that under mild condition these schemes can identify the local geometry associated to the solution. The benefits of such local geometry will be highlighted through computational complexity and first-/higher-order acceleration.
                      </p>
                  </div>
              </div>
              </div>

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Mar 6, 2018<span>12:00</span></div>
                  <div class="tracking-content">
                    <b> Coralia Cartis</b>
                    <span>University of Oxford and The Alan Turing Institute</span>
                    <b>Title</b>: Complexity of Nonconvex Optimization: The Story So Far.
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_5" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_5" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        Establishing the global rate of convergence of standard algorithms or their global evaluation complexity for nonconvex smooth optimization problems is a natural but challenging aspect of algorithm analysis. In the last decade, substantial progress has been made and continues to be made, in this direction. We review some of the key developments, illustrating the crucial role played by cubic regularisation methods, a credible alternative to linesearch and trust-region.
                      </p>
                      <p class="mbr-fonts-style panel-text display-7">
                        We then focus on two recent results. Firstly, we consider a general/new class of adaptive regularization methods, that use first- or higher-order local Taylor models of the objective regularized by a(ny) power of the step size. We investigate the worst-case evaluation complexity of these algorithms, when the level of sufficient smoothness of the objective and its derivatives may be unknown or may even be absent, and we find that the methods accurately reflect in their complexity the (unknown) degree of smoothness of the objective/derivatives and satisfy increasingly better bounds with the order of the derivatives available. We then focus on the antipodal context, when no derivatives of the objective are available, and the local models in adaptive cubic regularisation methods are constructed for example, by sampling of function values. We show that the evaluation complexity of the ensuing methods do not change in the order of the accuracy from their deterministic counterparts, increasing only by a constant factor which depends on the probability of the sampled models being occasionally, sufficiently accurate.
                      </p>
                      <p class="mbr-fonts-style panel-text display-7">
                        Time permitting, we will also discuss the stochastic case when even function evaluations may be inaccurate. This work is joint with Nick Gould (Rutherford Appleton Laboratory, UK), Philippe Toint (University of Namur, Belgium) and Katya Scheinberg (Lehigh University, USA).
                      </p>
                  </div>
              </div>
              </div>
            </div>
            <div class="tracking-list">

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Nov 28, 2017<span>13:00</span></div>
                  <div class="tracking-content">
                    <b> Adilet Otemissov</b>
                    <span>University of Oxford and The Alan Turing Institute</span>
                    <b>Title</b>: Dimensionality reduction techniques for global optimization.
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_4" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_4" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        (Joint work with Coralia Cartis) The problem of finding the most extreme value of a function, also known as global optimization, is a challenging task. The difficulty is associated with the exponential increase in the computational time for a linear increase in the dimension. This is known as the ``curse of dimensionality''. In this talk, we demonstrate that such challenges can be overcome for functions with low effective dimensionality – functions which are constant along certain linear subspaces. Such functions can often be found in applications, for example, in hyper-parameter optimization for neural networks, heuristic algorithms for combinatorial optimization problems and complex engineering simulations.
                      </p>
                      <p class="mbr-fonts-style panel-text display-7">
                        We propose the use of random subspace embeddings within a(ny) global minimisation algorithm, extending the approach in Wang et al. (2013). We introduce two techniques (REGO and AREGO) that transform the high-dimensional optimization problem into a low-dimensional problem. REGO is formulated by adding constraints in the low-dimensional embedded space and AREGO by including constraints in the original space. Using results from random matrix theory and conic integral geometry we derive probabilistic bounds on the success of the dimension-reduced problems. Finally, we present encouraging numerical results.
                      </p>
                  </div>
              </div>
              </div>

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Nov 14, 2017<span>13:00</span></div>
                  <div class="tracking-content">
                    <b> Bunel Rudy</b>
                    <span>University of Oxford</span>
                    <b>Title</b>: Formal Verification of Piecewise Linear Neural Network: A comparative Study
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_3" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_3" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        The success of Deep Learning and its potential use in many important safety critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure. This talk will present current strategies available to perform formal verification of models, as well as newly developped methods.
                      </p>
                  </div>
              </div>
              </div>

               <div class="tracking-item">
                  <div class="tracking-icon status-intransit">
                     <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                     </svg>
                     <!-- <i class="fas fa-circle"></i> -->
                  </div>
                  <div class="tracking-date">Oct 31, 2017<span>10:00</span></div>
                  <div class="tracking-content">
                    <b> Gianluca Detommaso</b>
                    <span>University of Bath and The Alan Turing Institute</span>
                    <b>Title</b>: Stein variational Quasi-Newton algorithm: sampling by sequential transport of particles.
                  </div>
                  <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_2" aria-expanded="false" aria-controls="collapse1">
                      <b>
                          <font color="blue"> Abstract </font>
                      </b>
                  </a>
               <div id="collapse1_2" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                  <div class="panel-body p-4">
                      <p class="mbr-fonts-style panel-text display-7">
                        In many statistical applications and real-world situations, it is of fundamental importance being able to quantify the uncertainty related to estimates of interest. However, whenever the underlying probability distribution is difficult or unknown, this cannot be done directly and sampling algorithms are typically needed. A recently introduced sampling algorithm is the <i> Stein variational gradient descent</i> [Q. Liu, D. Wang, 2016], where a cloud of particles are sequentially transported towards the target distribution. This is accomplished by a functional gradient descent which minimises the Kullback–Leibler divergence between the current distribution of the particles and the target one.
                      </p>
                      <p class="mbr-fonts-style panel-text display-7">
                        In collaboration with Dr. A. Spantini (MIT) and T. Cui (Monash, AU), we are currently working on accelerating this algorithm. From a transport maps perspective, we work out second-order information to replace gradient descent with Quasi-Newton algorithms, with potentially huge convergence accelerations. Furthermore, we substitute the simple kernel used in the original algorithm by more sophisticated ones, better representing the interaction between the particles and accelerating their spread along the target distribution support.
                      </p>
                  </div>
              </div>
              </div>

              <div class="tracking-item">
                 <div class="tracking-icon status-intransit">
                    <svg class="svg-inline--fa fa-circle fa-w-16" aria-hidden="true" data-prefix="fas" data-icon="circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg="">
                       <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8z"></path>
                    </svg>
                    <!-- <i class="fas fa-circle"></i> -->
                 </div>
                 <div class="tracking-date">Oct 18, 2017<span>09:00</span></div>
                 <div class="tracking-content"> <b> Pankaj Pansari </b> <span>University of Oxford and The Alan Turing Institute</span> <b>Title</b>: Optimal Submodular Extensions for Marginal Estimation.</div>
                 <a role="button" class="collapsed panel-title text-black" data-toggle="collapse" data-parent="#accordion" data-core="" href="#collapse1_1" aria-expanded="false" aria-controls="collapse1">
                     <b>
                         <font color="blue"> Abstract </font>
                     </b>
                 </a>
              <div id="collapse1_1" class="panel-collapse noScroll collapse" role="tabpanel" aria-labelledby="headingOne">
                 <div class="panel-body p-4">
                     <p class="mbr-fonts-style panel-text display-7">
                       In this talk, I will briefly summarize our recent work on using
                       submodular functions for variational inference. We show a connection
                       between submodular extension of energy functions and LP relaxations of
                       MAP estimation problem. This enables us to find worst-case optimal
                       submodular extensions for Potts and hierarchical Potts models.
                       Importantly, it helps us to use a fast Gaussian-filtering algorithm to
                       operationalize our algorithm for dense CRFs, providing the first
                       practical method to compute upper-bound on log-partition for such
                       models.
                     </p>
                 </div>
              </div>
              </div>
            </div>
         </div>
      </div>
   </div>
</div>

</section>

<section class="cid-r6LJ36CKFS" id="footer2-m">

    <div class="mbr-overlay" style="opacity: 0.5; background-color: rgb(60, 60, 60);"></div>

    <div class="container">
        <div class="media-container-row content mbr-white">
            <div class="col-12 col-md-3 mbr-fonts-style display-7">
                <p class="mbr-text">
                    <strong>Address</strong>
                    <br>
                    <br>The Alan Turing Institute
                    <br>96 Euston Road, London, NW1 2DB
                    <br>
                    <br><strong>Contacts</strong>
                    <br>
                    <br>Email: toc_head@turing.ac.uk &nbsp; &nbsp; &nbsp; Phone: +44 (0) 7378 173765
                </p>
            </div>
            <div class="col-12 col-md-3 mbr-fonts-style display-7">
                <p class="mbr-text"><strong>Feedback</strong>
                    <br>
                    <br>We are happy to receive your ideas and suggestions! Any feedback would be appreciated.
                </p>
            </div>
            <div class="col-12 col-md-6">
                <div class="google-map"><iframe frameborder="0" style="border:0" src="https://www.google.com/maps/embed/v1/place?key=AIzaSyA0Dx_boXQiwvdz8sJHoYeZNVTdoWONYkU&amp;q=place_id:ChIJ19W7bjsbdkgRgCE8zgx3CdY" allowfullscreen=""></iframe></div>
            </div>
        </div>

    </div>
</section>


  <script src="assets/web/assets/jquery/jquery.min.js"></script>
  <script src="assets/popper/popper.min.js"></script>
  <script src="assets/tether/tether.min.js"></script>
  <script src="assets/bootstrap/js/bootstrap.min.js"></script>
  <script src="assets/smoothscroll/smooth-scroll.js"></script>
  <script src="assets/parallax/jarallax.min.js"></script>
  <script src="assets/mbr-switch-arrow/mbr-switch-arrow.js"></script>
  <script src="assets/theme/js/script.js"></script>


</body>
</html>
